{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNki4bnUNdWmg70RRC70r6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokeshtalamala1/Deep_Learning_Assignment_1/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing my project and entity in wandb using its atributes\n",
        "#Deep_Learning_Assignment_1\n",
        "#cs24m023-indian-institute-of-technology-madras\n",
        "\n",
        "!pip install wandb tensorflow\n",
        "import wandb\n",
        "wandb.init(project='Deep_Learning_Assignment_1', entity='cs24m023-indian-institute-of-technology-madras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ry9MQcbX4y_F",
        "outputId": "8f155c54-17c3-43bf-b679-cd63d314c015"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/runs/2ackpq00?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7dd9b6186cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy ,fashion_mnist and load data\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "((x_train,y_train),(x_test,y_test)) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "IAODjbNR42bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15df4f8-0e84-43c0-bfe2-fcbcbee5d943"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Zec4K1i87JU8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example image from each class from given data\n",
        "\n",
        "#QUESTION 1\n",
        "\n",
        "images = []\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "for class_label in range(len(classes)):\n",
        "    index = next(i for i, y in enumerate(y_train) if y == class_label)\n",
        "    images.append(wandb.Image(x_train[index], caption=classes[class_label]))\n",
        "\n",
        "wandb.log({\"example image from each class\": images})\n",
        "plt.show()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ZEliQ5pw7Npi",
        "outputId": "e0f18cf4-5a0e-4154-ec5a-7727b794be4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glowing-firefly-55</strong> at: <a href='https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/runs/2ackpq00' target=\"_blank\">https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/runs/2ackpq00</a><br> View project at: <a href='https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1' target=\"_blank\">https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_194826-2ackpq00/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 2 AND QUESTION 3\n",
        "#creating a class with a layer as an object and parametes to the layer are :\n",
        "    #input dimensions\n",
        "    #activation_function\n",
        "    #optimizer_function\n",
        "    #weight initialization\n",
        "\n",
        "#FORWARD PROPAGATE AND BACKWARD PROPAGATE\n",
        "#feedforwardneuralnetwork for calculating all the layers over the model and loss functions are calculated\n",
        "#with input parameters optimizer,activation function weight initialization\n",
        "#number of epochs and size of each layer ,learnig rate theeta\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# -------------------- FeedForwardNN Class --------------------\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, input_size=None, hidden_nodes=None, output_size=None, layer_dims=None,\n",
        "                 activation_hidden='relu', init_type='xavier'):\n",
        "        \"\"\"\n",
        "        Initializes the neural network.\n",
        "\n",
        "        You can specify the architecture in two ways:\n",
        "          1. Directly pass a list of layer dimensions as 'layer_dims'.\n",
        "          2. Or pass the input size, list of hidden layer node counts, and output size.\n",
        "        \"\"\"\n",
        "        if layer_dims is not None:\n",
        "            self.layer_dims = layer_dims\n",
        "        else:\n",
        "            if input_size is None or hidden_nodes is None or output_size is None:\n",
        "                raise ValueError(\"Either layer_dims or input_size, hidden_nodes, and output_size must be provided\")\n",
        "            self.layer_dims = [input_size] + hidden_nodes + [output_size]\n",
        "\n",
        "        self.L = len(self.layer_dims) - 1  # Number of layers excluding the input layer\n",
        "        self.init_type = init_type\n",
        "        self.activation_hidden = activation_hidden\n",
        "        self.parameters = self.initialize_parameters(self.layer_dims, init_type)\n",
        "\n",
        "    # ---- Activation Functions and Their Gradients ----\n",
        "    def sigmoid(z):\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "    def dsigmoid(z):\n",
        "        s = FeedForwardNN.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def tanh(z):\n",
        "        return np.tanh(z)\n",
        "\n",
        "    def dtanh(z):\n",
        "        return 1 - np.tanh(z)**2\n",
        "\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def drelu(z):\n",
        "        return np.array(z > 0, dtype=float)\n",
        "\n",
        "    def softmax(z):\n",
        "        z_exp = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "        return z_exp / np.sum(z_exp, axis=0, keepdims=True)\n",
        "\n",
        "    def dsoftmax(z):\n",
        "        s = FeedForwardNN.softmax(z.reshape(-1, 1))\n",
        "        jacobian = np.diagflat(s) - np.dot(s, s.T)\n",
        "        return jacobian\n",
        "\n",
        "    def get_activation_function(self):\n",
        "        if self.activation_hidden.lower() == 'sigmoid':\n",
        "            return FeedForwardNN.sigmoid, FeedForwardNN.dsigmoid\n",
        "        elif self.activation_hidden.lower() == 'tanh':\n",
        "            return FeedForwardNN.tanh, FeedForwardNN.dtanh\n",
        "        elif self.activation_hidden.lower() == 'relu':\n",
        "            return FeedForwardNN.relu, FeedForwardNN.drelu\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'sigmoid', 'tanh', or 'relu'.\")\n",
        "\n",
        "    # ---- Parameter Initialization ----\n",
        "    def initialize_parameters(self, layer_dims, init_type='random'):\n",
        "        parameters = {}\n",
        "        L = len(layer_dims)\n",
        "        for l in range(1, L):\n",
        "            if init_type.lower() == 'random':\n",
        "                parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
        "            elif init_type.lower() == 'xavier':\n",
        "                parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1.0 / layer_dims[l-1])\n",
        "            else:\n",
        "                raise ValueError(\"init_type must be 'random' or 'xavier'\")\n",
        "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        return parameters\n",
        "\n",
        "    # ---- Forward Propagation ----\n",
        "    def forward_propagation(self, X):\n",
        "        caches = []\n",
        "        A = X\n",
        "        activation_func, _ = self.get_activation_function()\n",
        "        # Process hidden layers\n",
        "        for l in range(1, self.L):\n",
        "            W = self.parameters['W' + str(l)]\n",
        "            b = self.parameters['b' + str(l)]\n",
        "            Z = np.dot(W, A) + b\n",
        "            A = activation_func(Z)\n",
        "            caches.append((Z, A))\n",
        "        # Output layer with softmax activation\n",
        "        W = self.parameters['W' + str(self.L)]\n",
        "        b = self.parameters['b' + str(self.L)]\n",
        "        Z = np.dot(W, A) + b\n",
        "        A_final = FeedForwardNN.softmax(Z)\n",
        "        caches.append((Z, A_final))\n",
        "        return A_final, caches\n",
        "\n",
        "    # ---- Backpropagation ----\n",
        "    def back_propagation(self, X, Y, caches, weight_decay=0.0):\n",
        "        grads = {}\n",
        "        m = X.shape[1]\n",
        "        L = self.L\n",
        "        _, activation_grad = self.get_activation_function()\n",
        "        # Output layer gradient (using softmax + cross-entropy)\n",
        "        ZL, AL = caches[-1]\n",
        "        dZL = AL - Y\n",
        "        WL_prev = self.parameters['W' + str(L)]\n",
        "        A_prev = caches[-2][1] if L > 1 else X\n",
        "        grads[\"dW\" + str(L)] = (np.dot(dZL, A_prev.T) / m) + weight_decay * self.parameters['W' + str(L)]\n",
        "        grads[\"db\" + str(L)] = np.sum(dZL, axis=1, keepdims=True) / m\n",
        "        dA_prev = np.dot(WL_prev.T, dZL)\n",
        "        # Backprop for hidden layers\n",
        "        for l in reversed(range(1, L)):\n",
        "            Z, A = caches[l-1]\n",
        "            dZ = dA_prev * activation_grad(Z)\n",
        "            A_prev = X if l == 1 else caches[l-2][1]\n",
        "            grads[\"dW\" + str(l)] = (np.dot(dZ, A_prev.T) / m) + weight_decay * self.parameters['W' + str(l)]\n",
        "            grads[\"db\" + str(l)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "            if l > 1:\n",
        "                W_prev = self.parameters['W' + str(l)]\n",
        "                dA_prev = np.dot(W_prev.T, dZ)\n",
        "        return grads\n",
        "\n",
        "    # ---- Compute Loss and Accuracy ----\n",
        "    def compute_loss_and_accuracy(self, X, Y):\n",
        "        A, _ = self.forward_propagation(X)\n",
        "        m = X.shape[1]\n",
        "        loss = -np.sum(Y * np.log(A + 1e-8)) / m\n",
        "        predictions = np.argmax(A, axis=0)\n",
        "        labels = np.argmax(Y, axis=0)\n",
        "        accuracy = np.mean(predictions == labels)\n",
        "        return loss, accuracy\n",
        "\n",
        "    # ---- Training Step (Mini-Batch) ----\n",
        "    def train_step(self, X_batch, Y_batch, optimizer, weight_decay, t=1):\n",
        "        A_final, caches = self.forward_propagation(X_batch)\n",
        "        m = X_batch.shape[1]\n",
        "        loss = -np.sum(Y_batch * np.log(A_final + 1e-8)) / m\n",
        "        grads = self.back_propagation(X_batch, Y_batch, caches, weight_decay=weight_decay)\n",
        "        self.parameters = optimizer.update_parameters(self.parameters, grads, t)\n",
        "        return loss\n",
        "\n",
        "    # ---- Training Loop with Mini-Batches ----\n",
        "    def train(self, X_train, Y_train, X_val, Y_val, optimizer, epochs=10, batch_size=32, weight_decay=0.0):\n",
        "        m = X_train.shape[1]\n",
        "        t = 1  # time step for optimizers (e.g., Adam)\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X_train[:, permutation]\n",
        "            Y_shuffled = Y_train[:, permutation]\n",
        "            epoch_loss = 0\n",
        "            num_batches = m // batch_size\n",
        "            for i in range(num_batches):\n",
        "                start = i * batch_size\n",
        "                end = start + batch_size\n",
        "                X_batch = X_shuffled[:, start:end]\n",
        "                Y_batch = Y_shuffled[:, start:end]\n",
        "                loss = self.train_step(X_batch, Y_batch, optimizer, weight_decay, t)\n",
        "                epoch_loss += loss\n",
        "                t += 1\n",
        "            epoch_loss /= num_batches\n",
        "            train_loss, train_acc = self.compute_loss_and_accuracy(X_train, Y_train)\n",
        "            val_loss, val_acc = self.compute_loss_and_accuracy(X_val, Y_val)\n",
        "            print(f\"Epoch {epoch:03d}: Epoch Loss = {epoch_loss:.4f} | Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f} | Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "        # Optionally, metrics can be returned or logged externally.\n",
        "\n",
        "\n",
        "# -------------------- Optimizers Class --------------------\n",
        "class Optimizers:\n",
        "    def __init__(self, method='sgd', learning_rate=0.01, momentum=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, decay=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the optimizer.\n",
        "\n",
        "        Supported methods: 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'\n",
        "        \"\"\"\n",
        "        self.method = method.lower()\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.decay = decay\n",
        "        self.v = {}\n",
        "        self.s = {}\n",
        "\n",
        "    def initialize_state(self, parameters):\n",
        "        for key in parameters:\n",
        "            self.v[key] = np.zeros_like(parameters[key])\n",
        "            self.s[key] = np.zeros_like(parameters[key])\n",
        "\n",
        "    def update_parameters(self, parameters, grads, t=1):\n",
        "        if not self.v or not self.s:\n",
        "            self.initialize_state(parameters)\n",
        "        updated_parameters = {}\n",
        "        for key in parameters.keys():\n",
        "            grad = grads.get(\"d\" + key, None)\n",
        "            if grad is None:\n",
        "                updated_parameters[key] = parameters[key]\n",
        "                continue\n",
        "            if self.method == 'sgd':\n",
        "                update = self.lr * grad\n",
        "            elif self.method == 'momentum':\n",
        "                self.v[key] = self.momentum * self.v[key] + self.lr * grad\n",
        "                update = self.v[key]\n",
        "            elif self.method == 'nesterov':\n",
        "                v_prev = self.v[key].copy()\n",
        "                self.v[key] = self.momentum * self.v[key] + self.lr * grad\n",
        "                update = self.momentum * v_prev + (1 + self.momentum) * self.lr * grad\n",
        "            elif self.method == 'rmsprop':\n",
        "                self.s[key] = self.beta2 * self.s[key] + (1 - self.beta2) * (grad ** 2)\n",
        "                update = self.lr * grad / (np.sqrt(self.s[key]) + self.epsilon)\n",
        "            elif self.method == 'adam':\n",
        "                self.v[key] = self.beta1 * self.v[key] + (1 - self.beta1) * grad\n",
        "                self.s[key] = self.beta2 * self.s[key] + (1 - self.beta2) * (grad ** 2)\n",
        "                v_corrected = self.v[key] / (1 - self.beta1**t)\n",
        "                s_corrected = self.s[key] / (1 - self.beta2**t)\n",
        "                update = self.lr * v_corrected / (np.sqrt(s_corrected) + self.epsilon)\n",
        "            elif self.method == 'nadam':\n",
        "                self.v[key] = self.beta1 * self.v[key] + (1 - self.beta1) * grad\n",
        "                self.s[key] = self.beta2 * self.s[key] + (1 - self.beta2) * (grad ** 2)\n",
        "                v_corrected = self.v[key] / (1 - self.beta1**t)\n",
        "                s_corrected = self.s[key] / (1 - self.beta2**t)\n",
        "                update = self.lr * (self.beta1 * v_corrected + (1 - self.beta1) * grad / (1 - self.beta1**t)) / (np.sqrt(s_corrected) + self.epsilon)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported optimization method.\")\n",
        "            updated_parameters[key] = parameters[key] - update\n",
        "        return updated_parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fUS56zKsgWbe"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}