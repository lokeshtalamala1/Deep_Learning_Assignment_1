{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1n70hFMs9GT7V6+Q8ku8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokeshtalamala1/Deep_Learning_Assignment_1/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing my project and entity in wandb using its atributes\n",
        "#Deep Learning Assignment 1\n",
        "#CS24M023\n",
        "\n",
        "!pip install wandb tensorflow\n",
        "import wandb\n",
        "wandb.init(project='Deep_Learning_Assignment_1', entity='cs24m023-indian-institute-of-technology-madras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ry9MQcbX4y_F",
        "outputId": "65cd9191-bb8b-4f97-da59-43f83eba2dad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/runs/n4wvwt4h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7dfc155a71d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy ,fashion_mnist and load data\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "((x_train,y_train),(x_test,y_test)) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "IAODjbNR42bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ef09fe-770c-45b8-cb4a-90ed896c86a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Zec4K1i87JU8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example image from each class from given data\n",
        "\n",
        "#QUESTION 1\n",
        "\n",
        "images = []\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "for class_label in range(len(classes)):\n",
        "    index = next(i for i, y in enumerate(y_train) if y == class_label)\n",
        "    images.append(wandb.Image(x_train[index], caption=classes[class_label]))\n",
        "\n",
        "wandb.log({\"example image from each class\": images})\n",
        "plt.show()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ZEliQ5pw7Npi",
        "outputId": "025c33fc-d610-45bd-c733-2836b27ce820"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">silvery-universe-5</strong> at: <a href='https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/runs/n4wvwt4h' target=\"_blank\">https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/runs/n4wvwt4h</a><br> View project at: <a href='https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1' target=\"_blank\">https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_053325-n4wvwt4h/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a class with a layer as an object and parametes to the layer are :\n",
        "    #input dimensions\n",
        "    #activation_function\n",
        "    #optimizer_function\n",
        "    #weight initialization #QUESTION 2 AND QUESTION 3\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SingleLayer:\n",
        "    def __init__(self, input_dim, num_nodes, activation='softmax', optimizer='gradient_descent', weight_type='random'):\n",
        "        self.input_dim = input_dim\n",
        "        self.num_nodes = num_nodes\n",
        "        self.activation = activation\n",
        "        self.optimizer_name = optimizer\n",
        "        self.weights, self.bias = self.initialize_weights(weight_type)\n",
        "\n",
        "        # Momentum and velocity terms\n",
        "        self.momentum_w = np.zeros((num_nodes, input_dim))\n",
        "        self.momentum_b = np.zeros((num_nodes, 1))\n",
        "        self.velocity_w = np.zeros((num_nodes, input_dim))\n",
        "        self.velocity_b = np.zeros((num_nodes, 1))\n",
        "\n",
        "        # Function mappings\n",
        "        self.activation_func, self.activation_deriv = self.get_activation_function(activation)\n",
        "        self.optimizer_func = self.get_optimizer_function(optimizer)\n",
        "\n",
        "    def initialize_weights(self, weight_type):\n",
        "        np.random.seed(1)\n",
        "        if weight_type == 'random':\n",
        "            weights = np.random.normal(0, 0.5, (self.num_nodes, self.input_dim))\n",
        "        else:  # Xavier Initialization\n",
        "            limit = np.sqrt(1 / self.input_dim)\n",
        "            weights = np.random.uniform(-limit, limit, (self.num_nodes, self.input_dim))\n",
        "        bias = np.ones((self.num_nodes, 1))\n",
        "        return weights, bias\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        activations = {\n",
        "            'sigmoid': (self.sigmoid, self.sigmoid_derivative),\n",
        "            'relu': (self.relu, self.relu_derivative),\n",
        "            'tanh': (self.tanh, self.tanh_derivative),\n",
        "            'softmax': (self.softmax, self.softmax_derivative)\n",
        "        }\n",
        "        return activations.get(activation, activations['softmax'])\n",
        "\n",
        "    def get_optimizer_function(self, optimizer):\n",
        "        optimizers = {\n",
        "            'gradient_descent': self.gradient_descent,\n",
        "            'momentum_gradient_descent': self.momentum_gradient_descent,\n",
        "            'rmsprop': self.rmsprop,\n",
        "            'adam': self.adam,\n",
        "            'stochastic_gradient_descent': self.stochastic_gradient_descent,\n",
        "            'nadam': self.nadam,\n",
        "            'nesterov': self.nesterov\n",
        "        }\n",
        "        return optimizers.get(optimizer, self.gradient_descent)\n",
        "\n",
        "    # Activation Functions\n",
        "    def sigmoid(self, Z):\n",
        "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "\n",
        "    def sigmoid_derivative(self, A):\n",
        "        return A * (1 - A)\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, A):\n",
        "        return (A > 0).astype(float)\n",
        "\n",
        "    def tanh(self, Z):\n",
        "        return np.tanh(Z)\n",
        "\n",
        "    def tanh_derivative(self, A):\n",
        "        return 1 - A ** 2\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, A):\n",
        "        return A * (1 - A)\n",
        "\n",
        "    # Forward Propagation\n",
        "    def forward_propagate(self, A):\n",
        "        self.prev_A = A\n",
        "        Z = np.dot(self.weights, A) + self.bias\n",
        "        self.prev_Z = Z\n",
        "        return self.activation_func(Z)\n",
        "\n",
        "    # Backward Propagation\n",
        "    def backward_propagate(self, dA):\n",
        "        m = self.prev_A.shape[1]\n",
        "        dZ = self.activation_deriv(self.prev_Z) * dA\n",
        "        self.dW = np.dot(dZ, self.prev_A.T) / m\n",
        "        self.dB = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        return np.dot(self.weights.T, dZ)\n",
        "\n",
        "     # Optimizers\n",
        "    def gradient_descent(self, learn_rate, l2_lambda=0, batch_size=32):\n",
        "        self.weights -= learn_rate * (self.dW + l2_lambda / batch_size * self.weights)\n",
        "        self.bias -= learn_rate * self.dB\n",
        "\n",
        "    def momentum_gradient_descent(self, learn_rate, beta=0.9, l2_lambda=0, batch_size=32):\n",
        "        self.momentum_w = beta * self.momentum_w + learn_rate * self.dW\n",
        "        self.momentum_b = beta * self.momentum_b + learn_rate * self.dB\n",
        "        self.weights -= self.momentum_w + l2_lambda / batch_size * self.weights\n",
        "        self.bias -= self.momentum_b\n",
        "\n",
        "    def rmsprop(self, learn_rate, beta=0.9, epsilon=1e-8, l2_lambda=0, batch_size=32):\n",
        "        self.velocity_w = beta * self.velocity_w + (1 - beta) * np.square(self.dW)\n",
        "        self.velocity_b = beta * self.velocity_b + (1 - beta) * np.square(self.dB)\n",
        "        self.weights -= learn_rate * self.dW / (np.sqrt(self.velocity_w) + epsilon) + l2_lambda / batch_size * self.weights\n",
        "        self.bias -= learn_rate * self.dB / (np.sqrt(self.velocity_b) + epsilon)\n",
        "\n",
        "    def adam(self, learn_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
        "        self.momentum_w = beta1 * self.momentum_w + (1 - beta1) * self.dW\n",
        "        self.momentum_b = beta1 * self.momentum_b + (1 - beta1) * self.dB\n",
        "        self.velocity_w = beta2 * self.velocity_w + (1 - beta2) * np.square(self.dW)\n",
        "        self.velocity_b = beta2 * self.velocity_b + (1 - beta2) * np.square(self.dB)\n",
        "\n",
        "        corrected_mw = self.momentum_w / (1 - beta1 ** t)\n",
        "        corrected_mb = self.momentum_b / (1 - beta1 ** t)\n",
        "        corrected_vw = self.velocity_w / (1 - beta2 ** t)\n",
        "        corrected_vb = self.velocity_b / (1 - beta2 ** t)\n",
        "\n",
        "        self.weights -= learn_rate * corrected_mw / (np.sqrt(corrected_vw) + epsilon)\n",
        "        self.bias -= learn_rate * corrected_mb / (np.sqrt(corrected_vb) + epsilon)\n",
        "\n",
        "    def nesterov(self, learn_rate, beta=0.9):\n",
        "        temp_w = self.weights - beta * self.momentum_w\n",
        "        temp_b = self.bias - beta * self.momentum_b\n",
        "        self.weights -= learn_rate * self.dW + beta * temp_w\n",
        "        self.bias -= learn_rate * self.dB + beta * temp_b\n",
        "\n",
        "    def nadam(self, learn_rate, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
        "        mw_hat = (beta1 * self.momentum_w) / (1 - beta1 ** t) + self.dW\n",
        "        mb_hat = (beta1 * self.momentum_b) / (1 - beta1 ** t) + self.dB\n",
        "        vw_hat = (beta2 * self.velocity_w) / (1 - beta2 ** t)\n",
        "        vb_hat = (beta2 * self.velocity_b) / (1 - beta2 ** t)\n",
        "\n",
        "        self.weights -= learn_rate * mw_hat / (np.sqrt(vw_hat) + epsilon)\n",
        "        self.bias -= learn_rate * mb_hat / (np.sqrt(vb_hat) + epsilon)\n",
        "\n",
        "    # Prediction\n",
        "    def predict(self, A):\n",
        "        return self.forward_propagate(A)"
      ],
      "metadata": {
        "id": "cVKg9tfs7Rlf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 2 AND QUESTION 3 FORWARD PROPAGAE AND BACKWARD PROPAGATE\n",
        "#feedforwardneuralnetwork for calculating all the layers over the model and loss functions are calculated\n",
        "#with input parameters optimizer,activation function weight initialization\n",
        "#number of epochs and size of each layer ,learnig rate theeta\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(self, layers_size, epochs=5, learn_rate=0.001, l2_lambda=0, optimizer='gradient_descent',\n",
        "                 activation='sigmoid', weight_type='random', loss='cross_entropy'):\n",
        "        self.layers = []\n",
        "        self.layers_size = layers_size\n",
        "        self.epochs = epochs\n",
        "        self.learn_rate = learn_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.weight_type = weight_type\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.loss = loss\n",
        "\n",
        "        # Assigning loss functions\n",
        "        loss_functions = {\n",
        "            'mean_square': (self.mean_square, self.mean_square_grad),\n",
        "            'cross_entropy': (self.cross_entropy, self.cross_entropy_grad)\n",
        "        }\n",
        "        self.losscomputation, self.lossBackwardpass = loss_functions.get(loss, (None, None))\n",
        "\n",
        "        if self.losscomputation is None:\n",
        "            raise ValueError(\"Invalid loss function selected.\")\n",
        "\n",
        "    def addingLayer(self, input_dim=None, num_nodes=1, activation='', weight_type='random'):\n",
        "        \"\"\"Add a new layer to the neural network.\"\"\"\n",
        "        if not self.layers and input_dim is None:\n",
        "            raise ValueError('Invalid input dimensions for the first layer.')\n",
        "        input_dim = input_dim if input_dim is not None else self.layers[-1].outputDimension()\n",
        "        self.layers.append(SingleLayer(input_dim, num_nodes, activation, optimizer=self.optimizer, weight_type=weight_type))\n",
        "\n",
        "    # Loss functions and gradients\n",
        "    def mean_square(self, Y, A):\n",
        "        return np.mean(np.square(Y - A))\n",
        "\n",
        "    def mean_square_grad(self, Y, A):\n",
        "        return -2 * (Y - A)\n",
        "\n",
        "    def cross_entropy(self, Y, A):\n",
        "        return -np.mean(np.sum(Y * np.log(A + 1e-9), axis=0))\n",
        "\n",
        "    def cross_entropy_grad(self, Y, A):\n",
        "        return A - Y\n",
        "\n",
        "    def cost(self, Y, A):\n",
        "        \"\"\"Computes the loss for given true and predicted values.\"\"\"\n",
        "        return self.losscomputation(Y, A)\n",
        "\n",
        "    def forward_propagate(self, X):\n",
        "        \"\"\"Pass input through all layers of the network.\"\"\"\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward_propagate(X)\n",
        "        return X\n",
        "\n",
        "    def backward_propagate(self, Y, A):\n",
        "        \"\"\"Backpropagation step.\"\"\"\n",
        "        dA = self.lossBackwardpass(Y, A)\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward_propagate(dA)\n",
        "\n",
        "    def update_weights(self, batch_size, t=0):\n",
        "        \"\"\"Update the weights using the chosen optimizer.\"\"\"\n",
        "        for layer in self.layers:\n",
        "            layer.optimizer(self.learn_rate, l2_lambda=self.l2_lambda, batch_size=batch_size, t=t)\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test, batch_size=32):\n",
        "        \"\"\"Train the neural network using the provided dataset.\"\"\"\n",
        "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9, test_size=0.1, random_state=10)\n",
        "\n",
        "        if self.activation == 'relu':\n",
        "            self.weight_type = 'xavier'\n",
        "\n",
        "        # Add layers to the model\n",
        "        for i in range(1, len(self.layers_size) - 1):\n",
        "            self.addingLayer(input_dim=self.layers_size[i - 1], num_nodes=self.layers_size[i], activation=self.activation, weight_type=self.weight_type)\n",
        "\n",
        "        # Output layer (softmax)\n",
        "        self.addingLayer(input_dim=self.layers_size[-2], num_nodes=self.layers_size[-1], activation='softmax', weight_type=self.weight_type)\n",
        "\n",
        "        # One-hot encoding\n",
        "        y_train_encoded = np.eye(len(set(y_train)))[y_train].T\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Adjust learning rate if using specific optimizers\n",
        "            if self.activation == 'relu' and self.optimizer in {'momentum_gradient_descent', 'nesterov', 'rmsprop'}:\n",
        "                self.learn_rate /= 15\n",
        "\n",
        "            for i in range(0, x_train.shape[0], batch_size):\n",
        "                x_batch = x_train[i:i + batch_size]\n",
        "                y_batch = y_train_encoded[:, i:i + batch_size]\n",
        "\n",
        "                x_batch = x_batch.reshape(x_batch.shape[0], -1).T\n",
        "                x_batch = (x_batch - x_batch.min()) / (x_batch.max() - x_batch.min())\n",
        "\n",
        "                A = self.forward_propagate(x_batch)\n",
        "                self.backward_propagate(y_batch, A)\n",
        "                self.update_weights(batch_size=batch_size, t=epoch + 1)\n",
        "\n",
        "            # Evaluate after each epoch\n",
        "            val_loss, val_acc, _ = self.predict(x_val, y_val)\n",
        "            test_loss, test_acc, y_pred = self.predict(x_test, y_test)\n",
        "\n",
        "            print(f\"After {epoch + 1} iterations:\")\n",
        "            print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
        "            print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, x, y):\n",
        "        \"\"\"Predict outputs for the given input data.\"\"\"\n",
        "        x = x.reshape(x.shape[0], -1).T\n",
        "        y_encoded = np.eye(len(set(y)))[y].T\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.predict(x)\n",
        "\n",
        "        cross_entropy_loss = -np.mean(np.sum(y_encoded * np.log(x + 1e-9), axis=0))\n",
        "        y_pred = np.argmax(x, axis=0)\n",
        "        accuracy = np.mean(y == y_pred)\n",
        "\n",
        "        return cross_entropy_loss, accuracy, y_pred\n"
      ],
      "metadata": {
        "id": "wbXCK6N4eI5e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "sweep_config = {\n",
        "    'name': \"my-sweep\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'hidden_layer': {'values': [3, 4, 5]},\n",
        "        'hidden_size': {'values': [32, 64, 128]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'learn_rate': {'values': [1e-3, 1e-4]},\n",
        "        'optimizer': {\n",
        "            'values': ['momentum_gradient_descent', 'nesterov', 'rmsprop',\n",
        "                       'adam', 'nadam', 'stochastic_gradient_descent']\n",
        "        },\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'weight_initial': {'values': ['random', 'xavier']},\n",
        "        'activation': {'values': ['sigmoid', 'tanh', 'relu']}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initializing sweep with WandB\n",
        "sweep_id = wandb.sweep(sweep_config,\n",
        "                       entity=\"cs24m023-indian-institute-of-technology-madras\",\n",
        "                       project=\"Deep_Learning_Assignment_1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iDwqupCzG5v",
        "outputId": "1ae83566-2229-4000-ae18-750f4d6d662e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: cgk2fp87\n",
            "Sweep URL: https://wandb.ai/cs24m023-indian-institute-of-technology-madras/Deep_Learning_Assignment_1/sweeps/cgk2fp87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "omBz5KkpzMCw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}