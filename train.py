{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIEHc5mjng4dWeVSt/f/TD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokeshtalamala1/Deep_Learning_Assignment_1/blob/main/train_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbCallback\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Login to wandb with your provided key\n",
        "wandb.login(key='ad59fd6ee8f94be6bca41cbc7385976e9111be2b')\n",
        "\n",
        "# ----------------- Argument Parsing -----------------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-wp', '--wandb_project', type=str, default='Deep_Learning_Assignment_1',\n",
        "                    help='Project name used to track experiments in Weights & Biases dashboard')\n",
        "parser.add_argument('-we', '--wandb_entity', type=str, default='cs24m023-indian-institute-of-technology-madras',\n",
        "                    help='Wandb Entity used to track experiments in the Weights & Biases dashboard.')\n",
        "parser.add_argument('-d', '--dataset', type=str, default='fashion_mnist',\n",
        "                    choices=[\"mnist\", \"fashion_mnist\"],\n",
        "                    help='Dataset to use')\n",
        "parser.add_argument('-e', '--epochs', type=int, default=10, choices=[5, 10],\n",
        "                    help=\"Number of epochs to train neural network.\")\n",
        "parser.add_argument('-b', '--batch_size', type=int, default=64, choices=[16, 32, 64],\n",
        "                    help=\"Batch size used to train neural network.\")\n",
        "parser.add_argument('-l','--loss', type=str, default='cross_entropy', choices=[\"mean_square\", \"cross_entropy\"],\n",
        "                    help='Loss function to use')\n",
        "parser.add_argument('-o', '--optimizer', type=str, default='nadam',\n",
        "                    choices=[\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"],\n",
        "                    help='Optimizer to use')\n",
        "parser.add_argument('-lr', '--learning_rate', type=float, default=1e-03, choices=[1e-3, 1e-4],\n",
        "                    help='Learning rate used to optimize model parameters')\n",
        "parser.add_argument('-m', '--momentum', type=float, default=0.5,\n",
        "                    help='Momentum used by momentum and nag optimizers.')\n",
        "parser.add_argument('-beta', '--beta', type=float, default=0.5,\n",
        "                    help='Beta used by rmsprop optimizer')\n",
        "parser.add_argument('-beta1', '--beta1', type=float, default=0.5,\n",
        "                    help='Beta1 used by adam and nadam optimizers.')\n",
        "parser.add_argument('-beta2', '--beta2', type=float, default=0.5,\n",
        "                    help='Beta2 used by adam and nadam optimizers.')\n",
        "parser.add_argument('-eps', '--epsilon', type=float, default=0.000001,\n",
        "                    help='Epsilon used by optimizers.')\n",
        "parser.add_argument('-w_d', '--weight_decay', type=float, default=.0,\n",
        "                    choices=[0, 0.0005, 0.5],\n",
        "                    help='Weight decay used by optimizers.')\n",
        "parser.add_argument('-w_i', '--weight_init', type=str, default='random', choices=[\"random\", \"xavier\"],\n",
        "                    help='Weight initialization method')\n",
        "parser.add_argument('-nhl', '--num_layers', type=int, default=4, choices=[3, 4, 5],\n",
        "                    help='Number of hidden layers used in feedforward neural network.')\n",
        "parser.add_argument('-sz', '--hidden_size', type=int, default=128, choices=[32, 64, 128],\n",
        "                    help='Number of hidden neurons in a feedforward layer.')\n",
        "parser.add_argument('-a', '--activation', type=str, default='relu', choices=['sigmoid', 'tanh', 'relu'],\n",
        "                    help='Activation function to use')\n",
        "parser.add_argument('-oa', '--output_activation', type=str, default='softmax',\n",
        "                    choices=[\"softmax\"],\n",
        "                    help='Output activation function')\n",
        "# Using parse_known_args() to ignore extraneous notebook args like '-f ...'\n",
        "arguments, unknown = parser.parse_known_args()\n",
        "\n",
        "# Initialize wandb with parsed config\n",
        "wandb.init(project=arguments.wandb_project, entity=arguments.wandb_entity, config=vars(arguments))\n",
        "\n",
        "# ----------------- Data Loading -----------------\n",
        "if arguments.dataset == \"fashion_mnist\":\n",
        "    ((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()\n",
        "elif arguments.dataset == \"mnist\":\n",
        "    ((x_train, y_train), (x_test, y_test)) = mnist.load_data()\n",
        "\n",
        "# ----------------- Model Definitions -----------------\n",
        "\n",
        "class SingleLayer:\n",
        "    # SingleLayer object represents a layer with initialization, activation, and optimizer functions\n",
        "    def __init__(self, idim, nof_nodes, activation='', optimizer='gradient_descent', weight_type='random'):\n",
        "        self.optimizer = self.do_optimizer(optimizer)\n",
        "        self.opt = optimizer\n",
        "        self.activation, self.activationForwardFunction, self.activationBackwardFunction = self.do_activation(activation)\n",
        "        self.weights, self.bias = self.initialize(idim, nof_nodes, activation, weight_type=weight_type)\n",
        "        self.pv_weight, self.pv_bias = np.zeros([nof_nodes, idim]), np.zeros([nof_nodes, 1])\n",
        "        self.pm_weight, self.pm_bias = np.zeros([nof_nodes, idim]), np.zeros([nof_nodes, 1])\n",
        "\n",
        "    def initialize(self, nof_inputfeatures, nof_nodes, activation, weight_type):\n",
        "        np.random.seed(1)\n",
        "        if weight_type == 'random':\n",
        "            w = np.random.normal(0.0, 0.5, size=(nof_nodes, nof_inputfeatures))\n",
        "        else:\n",
        "            x = np.sqrt(nof_nodes)\n",
        "            w = np.random.uniform(-(1/x), (1/x), size=(nof_nodes, nof_inputfeatures))\n",
        "        b = np.ones([nof_nodes, 1])\n",
        "        return w, b\n",
        "\n",
        "    def do_optimizer(self, optimizer):\n",
        "        if optimizer == 'gradient_descent':\n",
        "            return self.gradient_descent\n",
        "        elif optimizer == 'momentum':\n",
        "            return self.momentum_gradient_descent\n",
        "        elif optimizer == 'rmsprop':\n",
        "            return self.rmsprop\n",
        "        elif optimizer == 'adam':\n",
        "            return self.adam\n",
        "        elif optimizer == 'sgd':\n",
        "            return self.stochastic_gradient_descent\n",
        "        elif optimizer == 'nadam':\n",
        "            return self.nadam\n",
        "        elif optimizer == 'nag':\n",
        "            return self.nesterov\n",
        "\n",
        "    def do_activation(self, activation):\n",
        "        if activation == 'sigmoid':\n",
        "            return activation, self.sigmoid, self.sigmoid_grad\n",
        "        elif activation == 'relu':\n",
        "            return activation, self.relu, self.relu_grad\n",
        "        elif activation == 'tanh':\n",
        "            return activation, self.tanh, self.tanh_grad\n",
        "        else:\n",
        "            return 'softmax', self.softmax, self.softmax_grad\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        Z = np.clip(Z, 500, -500)\n",
        "        A = 1 / (1 + np.exp(-Z))\n",
        "        return A\n",
        "\n",
        "    def sigmoid_grad(self, derivative_A):\n",
        "        e = np.exp(-self.previous_Z)\n",
        "        s = 1 / (1 + e)\n",
        "        derivative_Z = derivative_A * s * (1 - s)\n",
        "        return derivative_Z\n",
        "\n",
        "    def tanh(self, Z):\n",
        "        return np.tanh(Z)\n",
        "\n",
        "    def tanh_grad(self, derivative_A):\n",
        "        s = self.tanh(self.previous_Z)\n",
        "        return derivative_A * (1 - s**2)\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_grad(self, derivative_A):\n",
        "        return derivative_A * (self.previous_Z > 0)\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        maxZ = np.max(Z)\n",
        "        eZ = np.exp(Z - maxZ)\n",
        "        A = eZ / eZ.sum(axis=0, keepdims=True)\n",
        "        return A\n",
        "\n",
        "    def softmax_grad(self, derivative_A):\n",
        "        return derivative_A\n",
        "\n",
        "    def forward_propagate(self, A):\n",
        "        if self.opt != 'nesterov':\n",
        "            Z = np.dot(self.weights, A) + self.bias\n",
        "        else:\n",
        "            xw = self.weights - 0.9 * self.pv_weight\n",
        "            xb = self.bias - 0.9 * self.pv_bias\n",
        "            Z = np.dot(xw, A) + xb\n",
        "        self.previous_A = A\n",
        "        self.previous_Z = Z\n",
        "        A = self.activationForwardFunction(Z)\n",
        "        return A\n",
        "\n",
        "    def backward_propagate(self, derivative_A):\n",
        "        derivative_Z = self.activationBackwardFunction(derivative_A)\n",
        "        sp = self.previous_A.shape[1]\n",
        "        self.derivative_b = np.sum(derivative_Z, axis=1, keepdims=True) / sp\n",
        "        self.derivative_w = np.dot(derivative_Z, self.previous_A.T) / sp\n",
        "        return np.dot(self.weights.T, derivative_A)\n",
        "\n",
        "    def predict(self, A):\n",
        "        Z = np.dot(self.weights, A) + self.bias\n",
        "        A = self.activationForwardFunction(Z)\n",
        "        return A\n",
        "\n",
        "    def stochastic_gradient_descent(self, derivative_A, learn_rate=0.001, t=0, l2_lambda=0, batch_size=32):\n",
        "        derivative_Z = self.activationBackwardFunction(derivative_A)\n",
        "        a = self.previous_A.shape[1]\n",
        "        previous_derivative_A = np.dot(self.weights.T, derivative_Z)\n",
        "        for i in range(a):\n",
        "            b = derivative_Z[:, i:i+1]\n",
        "            self.derivative_b = b / a\n",
        "            self.derivative_w = np.dot(b, self.previous_A[:, i:i+1].T) / a\n",
        "            c = l2_lambda / batch_size\n",
        "            self.weights -= learn_rate * self.derivative_w - learn_rate * c * self.weights\n",
        "            self.bias -= learn_rate * self.derivative_b - c * self.bias\n",
        "        return previous_derivative_A\n",
        "\n",
        "    def rmsprop(self, learn_rate, t, l2_lambda=0, batch_size=32, mrate=0.9):\n",
        "        gws = np.square(self.derivative_w)\n",
        "        gbs = np.square(self.derivative_b)\n",
        "        nmrate = 1 - mrate\n",
        "        self.pv_weight = mrate * self.pv_weight + nmrate * gws\n",
        "        self.pv_bias = mrate * self.pv_bias + nmrate * gbs\n",
        "        self.pv_bias[self.pv_bias < 0] = 1e-9\n",
        "        self.pv_weight[self.pv_weight < 0] = 1e-9\n",
        "        a = learn_rate * l2_lambda / batch_size\n",
        "        self.weights = self.weights - a * self.weights\n",
        "        self.bias = self.bias - a * self.bias\n",
        "        b = np.sqrt(self.pv_bias + 1e-8)\n",
        "        c = learn_rate / b\n",
        "        self.weights = self.weights - c * self.derivative_w\n",
        "        self.bias = self.bias - c * self.derivative_b\n",
        "\n",
        "    def gradient_descent(self, learn_rate, l2_lambda=0, batch_size=32, t=0):\n",
        "        c = l2_lambda / batch_size\n",
        "        self.weights = self.weights - learn_rate * self.derivative_w - learn_rate * c * self.weights\n",
        "        self.bias = self.bias - learn_rate * self.derivative_b - c * self.bias\n",
        "\n",
        "    def momentum_gradient_descent(self, learn_rate, t, l2_lambda=0, batch_size=32, mrate=0.9):\n",
        "        c = l2_lambda / batch_size\n",
        "        self.pm_weight = mrate * self.pm_weight + learn_rate * self.derivative_w + c * self.weights\n",
        "        self.pm_bias = mrate * self.pm_bias + learn_rate * self.derivative_b + c * self.bias\n",
        "        self.weights -= self.pm_weight\n",
        "        self.bias -= self.pm_bias\n",
        "\n",
        "    def nesterov(self, learn_rate, mrate=0.9, l2_lambda=0, batch_size=32, t=0):\n",
        "        self.pv_weight = mrate * self.pv_weight + learn_rate * self.derivative_w\n",
        "        self.weights -= self.pv_weight\n",
        "        self.bias -= mrate * self.pv_bias\n",
        "\n",
        "    def adam(self, learn_rate, beta1=0.9, beta2=0.999, l2_lambda=0, batch_size=32, t=0):\n",
        "        nbeta1 = 1 - beta1\n",
        "        nbeta2 = 1 - beta2\n",
        "        self.pm_weight = beta1 * self.pm_weight + nbeta1 * self.derivative_w\n",
        "        self.pm_bias = beta1 * self.pm_bias + nbeta1 * self.derivative_b\n",
        "        sw = np.square(self.derivative_w)\n",
        "        sb = np.square(self.derivative_b)\n",
        "        self.pv_weight = beta2 * self.pv_weight + nbeta2 * sw\n",
        "        self.pv_bias = beta2 * self.pv_bias + nbeta2 * sb\n",
        "        pm_weightH = self.pm_weight / nbeta1\n",
        "        pm_biasH = self.pm_bias / nbeta1\n",
        "        pv_weightH = self.pv_weight / nbeta2\n",
        "        pv_biasH = self.pv_bias / nbeta2\n",
        "        rw = np.sqrt(pv_weightH + 1e-8)\n",
        "        self.weights = self.weights - learn_rate * (pm_weightH / rw)\n",
        "        rb = np.sqrt(pv_biasH + 1e-8)\n",
        "        self.bias = self.bias - learn_rate * (pm_biasH / rb)\n",
        "\n",
        "    def nadam(self, learn_rate, t, beta1=0.9, beta2=0.999, l2_lambda=0, batch_size=32):\n",
        "        nbeta1 = 1 - beta1\n",
        "        nbeta2 = 1 - beta2\n",
        "        self.pm_weight = beta1 * self.pm_weight + nbeta1 * self.derivative_w\n",
        "        self.pm_bias = beta1 * self.pm_bias + nbeta1 * self.derivative_b\n",
        "        sw = np.square(self.derivative_w)\n",
        "        sb = np.square(self.derivative_b)\n",
        "        self.pv_weight = beta2 * self.pv_weight + nbeta2 * sw\n",
        "        self.pv_bias = beta2 * self.pv_bias + nbeta2 * sb\n",
        "        pm_weightH = (beta1 * self.pm_weight / nbeta1) + self.derivative_w\n",
        "        pm_biasH = (beta1 * self.pm_bias / nbeta1) + self.derivative_b\n",
        "        pv_weightH = (beta2 * self.pv_weight) / nbeta2\n",
        "        pv_biasH = (beta2 * self.pv_bias) / nbeta2\n",
        "        aw = pm_weightH / (np.sqrt(pv_weightH + 1e-8))\n",
        "        ab = pm_biasH / (np.sqrt(pv_biasH + 1e-8))\n",
        "        self.weights -= learn_rate * aw\n",
        "        self.bias -= learn_rate * ab\n",
        "\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, layers_size, epochs=5, learning_rate=0.001, l2_lambda=0, optimizer='gradient_descent', activation='sigmoid', weight_type='random', loss='cross_entropy'):\n",
        "        self.layers = []\n",
        "        self.layers_size = layers_size\n",
        "        self.epochs = epochs\n",
        "        self.learn_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.weight_type = weight_type\n",
        "        self.l2_lambda = l2_lambda\n",
        "        if loss == 'mean_squared_error':\n",
        "            self.losscomputation = self.mean_square\n",
        "            self.lossBackwardpass = self.mean_square_grad\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.losscomputation = self.cross_entropy\n",
        "            self.lossBackwardpass = self.cross_entropy_grad\n",
        "        else:\n",
        "            print('loss computation is invalid')\n",
        "        self.loss = loss\n",
        "\n",
        "    def addingLayer(self, idim=None, nof_nodes=1, activation='', weight_type='random'):\n",
        "        if not self.layers:\n",
        "            if idim is None:\n",
        "                print('Invalid number of layers')\n",
        "        else:\n",
        "            if idim is None:\n",
        "                idim = self.layers[-1].outputDimension()\n",
        "        add_layer = SingleLayer(idim, nof_nodes, activation, optimizer=self.optimizer, weight_type=weight_type)\n",
        "        self.layers.append(add_layer)\n",
        "\n",
        "    def mean_square(self, Y, A):\n",
        "        l = np.square(Y - A)\n",
        "        c = np.sum(l) / Y.shape[1]\n",
        "        return np.squeeze(c)\n",
        "\n",
        "    def mean_square_grad(self, Y, A):\n",
        "        return -2 * (Y - A)\n",
        "\n",
        "    def cross_entropy(self, Y, A):\n",
        "        a = Y.shape[1]\n",
        "        b = np.sum(Y * np.log(A))\n",
        "        c = -(1/a) * b\n",
        "        return np.squeeze(c)\n",
        "\n",
        "    def cross_entropy_grad(self, Y, A):\n",
        "        return A - Y\n",
        "\n",
        "    def cost(self, Y, A):\n",
        "        return self.losscomputation(Y, A)\n",
        "\n",
        "    def forward_propagate(self, X):\n",
        "        result = np.copy(X)\n",
        "        for each_layer in self.layers:\n",
        "            result = each_layer.forward_propagate(result)\n",
        "        return result\n",
        "\n",
        "    def backward_propagate(self, Y, A):\n",
        "        derivative_A = self.lossBackwardpass(Y, A)\n",
        "        if self.optimizer != 'stochastic_gradient_descent':\n",
        "            for each_layer in reversed(self.layers):\n",
        "                derivative_A = each_layer.backward_propagate(derivative_A)\n",
        "        elif self.optimizer == 'stochastic_gradient_descent':\n",
        "            for each_layer in reversed(self.layers):\n",
        "                derivative_A = each_layer.stochastic_gradient_descent(derivative_A, learn_rate=self.learn_rate)\n",
        "        return derivative_A\n",
        "\n",
        "    def update_Weight(self, learning_rate=0.01, l2_lambda=0, batch_size=32, t=0):\n",
        "        for each_layer in self.layers:\n",
        "            each_layer.optimizer(learning_rate, l2_lambda=l2_lambda, batch_size=batch_size, t=t)\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test, batch_size=32):\n",
        "        # Split training data into training and validation sets\n",
        "        x, x_value, y, y_value = train_test_split(x_train, y_train, train_size=0.9, test_size=0.1, random_state=10)\n",
        "        # For relu activation, change weight initialization to Xavier\n",
        "        if self.activation == 'relu':\n",
        "            self.weight_type = 'xavier'\n",
        "        # Add hidden layers according to layers_size (except input & output)\n",
        "        l = len(self.layers_size)\n",
        "        for k in range(1, l - 1):\n",
        "            self.addingLayer(idim=self.layers_size[k-1], nof_nodes=self.layers_size[k], activation=self.activation, weight_type=self.weight_type)\n",
        "        # Add output layer with softmax activation\n",
        "        self.addingLayer(idim=self.layers_size[-2], nof_nodes=self.layers_size[-1], activation='softmax', weight_type=self.weight_type)\n",
        "\n",
        "        # One-hot encode labels for training data\n",
        "        i = len(y)\n",
        "        j = len(set(y))\n",
        "        y_encoder = np.zeros([j, i])\n",
        "        for k in range(i):\n",
        "            y_encoder[y[k]][k] = 1\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for i_epoch in range(self.epochs):\n",
        "            # Adjust learning rate for specific conditions\n",
        "            if self.activation == 'relu' and self.optimizer in ['momentum', 'nag', 'rmsprop']:\n",
        "                self.learn_rate = self.learn_rate / 15\n",
        "            # Process training data in batches\n",
        "            for k in range(0, x.shape[0], batch_size):\n",
        "                kbatch = k + batch_size\n",
        "                xbatch = x[k:kbatch]\n",
        "                ybatch = y[k:kbatch]\n",
        "                y_encoderbatch = y_encoder[:, k:kbatch]\n",
        "                # Reshape xbatch: (num_examples, 28, 28) to (784, num_examples)\n",
        "                xbatch = xbatch.reshape(xbatch.shape[0], -1).T\n",
        "                # Normalize xbatch\n",
        "                mi = np.min(xbatch)\n",
        "                mx = np.max(xbatch)\n",
        "                xbatch = (xbatch - mi) / (mx - mi + 1e-8)\n",
        "                # Forward and backward propagation\n",
        "                A = self.forward_propagate(xbatch)\n",
        "                self.backward_propagate(y_encoderbatch, A)\n",
        "                # Update weights if not using plain SGD\n",
        "                if self.optimizer != 'sgd':\n",
        "                    self.update_Weight(learning_rate=self.learn_rate, l2_lambda=self.l2_lambda, batch_size=batch_size, t=i_epoch+1)\n",
        "            # Evaluate on validation and test sets\n",
        "            validation_loss, validation_acc, _ = self.predict(x_value, y_value)\n",
        "            test_loss, test_acc, _ = self.predict(x_test, y_test)\n",
        "            print(\"After epoch\", i_epoch+1, \":\")\n",
        "            print(\"Validation loss:\", validation_loss, \"Validation accuracy:\", validation_acc)\n",
        "            print(\"Test loss:\", test_loss, \"Test accuracy:\", test_acc)\n",
        "            wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_acc,\n",
        "                       \"loss\": test_loss, \"accuracy\": test_acc, \"epoch\": i_epoch})\n",
        "        # Return the prediction probabilities on test set\n",
        "        return self.predict(x_test, y_test)[2]\n",
        "\n",
        "    def predict(self, x, y):\n",
        "        num_examples = x.shape[0]\n",
        "        x_reshaped = x.reshape(num_examples, -1).T\n",
        "        num_classes = len(set(y))\n",
        "        y_encoder = np.zeros([num_classes, num_examples])\n",
        "        for k in range(num_examples):\n",
        "            y_encoder[y[k]][k] = 1\n",
        "        A = x_reshaped\n",
        "        for each_layer in self.layers:\n",
        "            A = each_layer.predict(A)\n",
        "        loss_vector = -(y_encoder * np.log(A))\n",
        "        cross_entropy = loss_vector.mean() * num_classes\n",
        "        y_pred = np.argmax(A, axis=0)\n",
        "        accuracy = (y == y_pred).mean()\n",
        "        return cross_entropy, accuracy, A\n",
        "\n",
        "# ----------------- Model Instantiation and Training -----------------\n",
        "\n",
        "# Set a fixed seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "model = FeedForwardNN(\n",
        "    layers_size=[784] + [arguments.hidden_size] * arguments.num_layers + [10],\n",
        "    epochs=arguments.epochs,\n",
        "    learning_rate=arguments.learning_rate,\n",
        "    l2_lambda=arguments.weight_decay,\n",
        "    loss=arguments.loss,\n",
        "    activation=arguments.activation,\n",
        "    optimizer=arguments.optimizer,\n",
        "    weight_type=arguments.weight_init\n",
        ")\n",
        "\n",
        "# Train the model using the provided batch size\n",
        "y_pred = model.fit(x_train, y_train, x_test, y_test, batch_size=arguments.batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "r1HlX7kXOqOt",
        "outputId": "be7057af-f8fb-4e46-8a70-a43208d81c20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "<ipython-input-6-e8983c64ca84>:142: RuntimeWarning: invalid value encountered in divide\n",
            "  A = eZ / eZ.sum(axis=0, keepdims=True)\n",
            "<ipython-input-6-e8983c64ca84>:388: RuntimeWarning: divide by zero encountered in log\n",
            "  loss_vector = -(y_encoder * np.log(A))\n",
            "<ipython-input-6-e8983c64ca84>:388: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss_vector = -(y_encoder * np.log(A))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After epoch 1 :\n",
            "Validation loss: nan Validation accuracy: 0.2875\n",
            "Test loss: nan Test accuracy: 0.4055\n",
            "After epoch 2 :\n",
            "Validation loss: nan Validation accuracy: 0.288\n",
            "Test loss: nan Test accuracy: 0.4422\n",
            "After epoch 3 :\n",
            "Validation loss: nan Validation accuracy: 0.285\n",
            "Test loss: nan Test accuracy: 0.4595\n",
            "After epoch 4 :\n",
            "Validation loss: nan Validation accuracy: 0.26966666666666667\n",
            "Test loss: nan Test accuracy: 0.3977\n",
            "After epoch 5 :\n",
            "Validation loss: nan Validation accuracy: 0.26616666666666666\n",
            "Test loss: nan Test accuracy: 0.3637\n",
            "After epoch 6 :\n",
            "Validation loss: nan Validation accuracy: 0.2831666666666667\n",
            "Test loss: nan Test accuracy: 0.3178\n",
            "After epoch 7 :\n",
            "Validation loss: nan Validation accuracy: 0.285\n",
            "Test loss: nan Test accuracy: 0.2773\n",
            "After epoch 8 :\n",
            "Validation loss: nan Validation accuracy: 0.26266666666666666\n",
            "Test loss: nan Test accuracy: 0.2288\n",
            "After epoch 9 :\n",
            "Validation loss: nan Validation accuracy: 0.21566666666666667\n",
            "Test loss: nan Test accuracy: 0.1716\n",
            "After epoch 10 :\n",
            "Validation loss: nan Validation accuracy: 0.16233333333333333\n",
            "Test loss: nan Test accuracy: 0.1202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K18GEhBqQklt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
